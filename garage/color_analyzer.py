#!/usr/bin/env python3
"""
Ditto TalkingHead ìƒ‰ìƒ ë¹„êµ ë¶„ì„ ë„êµ¬
ì›ë³¸ base.mp4ì™€ ìƒì„±ëœ ë¹„ë””ì˜¤ì˜ ìƒ‰ìƒì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
from typing import Tuple, Dict, List
import json
from datetime import datetime

class DittoColorAnalyzer:
    def __init__(self, project_root: str = "/data/edutem/deokjin/ai-human/ditto-talkinghead"):
        """
        ì´ˆê¸°í™”
        Args:
            project_root: Ditto í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        """
        self.project_root = Path(project_root)
        self.base_video = self.project_root / "example" / "base.mp4"
        self.output_dir = self.project_root / "color_analysis"
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ¨ Ditto Color Analyzer ì´ˆê¸°í™”")
        print(f"ğŸ“ í”„ë¡œì íŠ¸ ê²½ë¡œ: {self.project_root}")
        print(f"ğŸ“¹ ê¸°ì¤€ ë¹„ë””ì˜¤: {self.base_video}")
    
    def extract_frames(self, video_path: str, num_frames: int = 5) -> List[np.ndarray]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì—¬ëŸ¬ í”„ë ˆì„ ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            num_frames: ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
        
        Returns:
            í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ (RGB í˜•ì‹)
        """
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
        frames = []
        
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                # BGR to RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
        
        cap.release()
        print(f"âœ… {len(frames)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: {Path(video_path).name}")
        return frames
    
    def calculate_color_stats(self, image: np.ndarray) -> Dict:
        """
        ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ í†µê³„ ê³„ì‚°
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (RGB)
        
        Returns:
            ìƒ‰ìƒ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        stats = {}
        
        # RGB í†µê³„
        for i, channel in enumerate(['R', 'G', 'B']):
            channel_data = image[:, :, i]
            stats[f'{channel}_mean'] = float(np.mean(channel_data))
            stats[f'{channel}_std'] = float(np.std(channel_data))
            stats[f'{channel}_min'] = float(np.min(channel_data))
            stats[f'{channel}_max'] = float(np.max(channel_data))
        
        # HSV í†µê³„
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        for i, channel in enumerate(['H', 'S', 'V']):
            channel_data = hsv[:, :, i]
            stats[f'{channel}_mean'] = float(np.mean(channel_data))
            stats[f'{channel}_std'] = float(np.std(channel_data))
        
        # ë°ê¸° (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        stats['brightness_mean'] = float(np.mean(gray))
        stats['brightness_std'] = float(np.std(gray))
        
        return stats
    
    def calculate_histogram(self, image: np.ndarray, bins: int = 256) -> Dict:
        """
        íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (RGB)
            bins: íˆìŠ¤í† ê·¸ë¨ ë¹ˆ ìˆ˜
        
        Returns:
            ê° ì±„ë„ì˜ íˆìŠ¤í† ê·¸ë¨
        """
        histograms = {}
        
        # RGB íˆìŠ¤í† ê·¸ë¨
        for i, channel in enumerate(['R', 'G', 'B']):
            hist, _ = np.histogram(image[:, :, i], bins=bins, range=(0, 256))
            histograms[channel] = hist
        
        # ë°ê¸° íˆìŠ¤í† ê·¸ë¨
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        hist, _ = np.histogram(gray, bins=bins, range=(0, 256))
        histograms['brightness'] = hist
        
        return histograms
    
    def compare_videos(self, generated_video_path: str, num_frames: int = 5):
        """
        ì›ë³¸ê³¼ ìƒì„±ëœ ë¹„ë””ì˜¤ ë¹„êµ
        
        Args:
            generated_video_path: ìƒì„±ëœ ë¹„ë””ì˜¤ ê²½ë¡œ
            num_frames: ë¹„êµí•  í”„ë ˆì„ ìˆ˜
        """
        print("\n" + "="*60)
        print("ğŸ” ìƒ‰ìƒ ë¹„êµ ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # í”„ë ˆì„ ì¶”ì¶œ
        original_frames = self.extract_frames(str(self.base_video), num_frames)
        generated_frames = self.extract_frames(generated_video_path, num_frames)
        
        # ì „ì²´ í†µê³„
        all_stats = {
            'original': [],
            'generated': [],
            'differences': []
        }
        
        # ê° í”„ë ˆì„ ìŒ ë¹„êµ
        for i, (orig, gen) in enumerate(zip(original_frames, generated_frames)):
            print(f"\nğŸ“Š í”„ë ˆì„ {i+1}/{num_frames} ë¶„ì„ ì¤‘...")
            
            # í†µê³„ ê³„ì‚°
            orig_stats = self.calculate_color_stats(orig)
            gen_stats = self.calculate_color_stats(gen)
            
            # ì°¨ì´ ê³„ì‚°
            diff_stats = {}
            for key in orig_stats:
                diff_stats[key] = gen_stats[key] - orig_stats[key]
            
            all_stats['original'].append(orig_stats)
            all_stats['generated'].append(gen_stats)
            all_stats['differences'].append(diff_stats)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_analysis_results(all_stats)
        
        # ì‹œê°í™”
        self._create_visualizations(original_frames, generated_frames, all_stats)
        
        # JSONìœ¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = self.output_dir / f"color_analysis_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_stats, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_path}")
    
    def _print_analysis_results(self, stats: Dict):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ìƒ‰ìƒ ë³€í™” ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        # í‰ê·  ì°¨ì´ ê³„ì‚°
        avg_diff = {}
        for diff in stats['differences']:
            for key, value in diff.items():
                if key not in avg_diff:
                    avg_diff[key] = []
                avg_diff[key].append(value)
        
        for key in avg_diff:
            avg_diff[key] = np.mean(avg_diff[key])
        
        # RGB ì±„ë„ë³„ ë³€í™”
        print("\nğŸ¨ RGB ì±„ë„ í‰ê·  ë³€í™”:")
        for channel in ['R', 'G', 'B']:
            mean_diff = avg_diff[f'{channel}_mean']
            std_diff = avg_diff[f'{channel}_std']
            print(f"  {channel}: í‰ê·  {mean_diff:+.2f}, í‘œì¤€í¸ì°¨ {std_diff:+.2f}")
        
        # HSV ë³€í™”
        print("\nğŸŒˆ HSV ì±„ë„ í‰ê·  ë³€í™”:")
        print(f"  H (ìƒ‰ìƒ): {avg_diff['H_mean']:+.2f}")
        print(f"  S (ì±„ë„): {avg_diff['S_mean']:+.2f}")
        print(f"  V (ëª…ë„): {avg_diff['V_mean']:+.2f}")
        
        # ë°ê¸° ë³€í™”
        print(f"\nğŸ’¡ ì „ì²´ ë°ê¸° ë³€í™”: {avg_diff['brightness_mean']:+.2f}")
        
        # ìƒ‰ìƒ í¸í–¥ ì§„ë‹¨
        print("\nğŸ” ì§„ë‹¨:")
        rgb_shifts = [avg_diff['R_mean'], avg_diff['G_mean'], avg_diff['B_mean']]
        max_shift = max(abs(s) for s in rgb_shifts)
        
        if max_shift > 20:
            print("  âš ï¸ ì‹¬ê°í•œ ìƒ‰ìƒ ë³€í™” ê°ì§€ë¨!")
        elif max_shift > 10:
            print("  âš¡ ì¤‘ê°„ ì •ë„ì˜ ìƒ‰ìƒ ë³€í™” ê°ì§€ë¨")
        elif max_shift > 5:
            print("  ğŸ“Œ ê²½ë¯¸í•œ ìƒ‰ìƒ ë³€í™” ê°ì§€ë¨")
        else:
            print("  âœ… ìƒ‰ìƒì´ ì˜ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤")
        
        # ê°€ì¥ í° ë³€í™”
        dominant_shift = max(enumerate(rgb_shifts), key=lambda x: abs(x[1]))
        channels = ['ë¹¨ê°•', 'ì´ˆë¡', 'íŒŒë‘']
        if abs(dominant_shift[1]) > 5:
            direction = "ì¦ê°€" if dominant_shift[1] > 0 else "ê°ì†Œ"
            print(f"  â†’ {channels[dominant_shift[0]]} ì±„ë„ì´ ê°€ì¥ í¬ê²Œ {direction} ({dominant_shift[1]:+.2f})")
    
    def _create_visualizations(self, original_frames: List, generated_frames: List, stats: Dict):
        """ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # 1. í”„ë ˆì„ ë¹„êµ
        fig, axes = plt.subplots(2, len(original_frames), figsize=(15, 6))
        fig.suptitle('ì›ë³¸ vs ìƒì„±ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ ë¹„êµ', fontsize=16)
        
        for i, (orig, gen) in enumerate(zip(original_frames, generated_frames)):
            axes[0, i].imshow(orig)
            axes[0, i].set_title(f'ì›ë³¸ í”„ë ˆì„ {i+1}')
            axes[0, i].axis('off')
            
            axes[1, i].imshow(gen)
            axes[1, i].set_title(f'ìƒì„± í”„ë ˆì„ {i+1}')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        frame_comp_path = self.output_dir / "frame_comparison.png"
        plt.savefig(frame_comp_path, dpi=150)
        plt.close()
        
        # 2. íˆìŠ¤í† ê·¸ë¨ ë¹„êµ (ì²« í”„ë ˆì„)
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¹„êµ (ì²« í”„ë ˆì„)', fontsize=16)
        
        orig_hist = self.calculate_histogram(original_frames[0])
        gen_hist = self.calculate_histogram(generated_frames[0])
        
        # RGB íˆìŠ¤í† ê·¸ë¨
        colors = ['red', 'green', 'blue']
        for i, (channel, color) in enumerate(zip(['R', 'G', 'B'], colors)):
            axes[0, 0].plot(orig_hist[channel], color=color, alpha=0.7, label=f'{channel} (ì›ë³¸)')
            axes[0, 0].plot(gen_hist[channel], color=color, alpha=0.7, linestyle='--', label=f'{channel} (ìƒì„±)')
        axes[0, 0].set_title('RGB íˆìŠ¤í† ê·¸ë¨')
        axes[0, 0].set_xlabel('í”½ì…€ ê°’')
        axes[0, 0].set_ylabel('ë¹ˆë„')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ë°ê¸° íˆìŠ¤í† ê·¸ë¨
        axes[0, 1].plot(orig_hist['brightness'], color='black', alpha=0.7, label='ì›ë³¸')
        axes[0, 1].plot(gen_hist['brightness'], color='black', alpha=0.7, linestyle='--', label='ìƒì„±')
        axes[0, 1].set_title('ë°ê¸° íˆìŠ¤í† ê·¸ë¨')
        axes[0, 1].set_xlabel('í”½ì…€ ê°’')
        axes[0, 1].set_ylabel('ë¹ˆë„')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # ìƒ‰ìƒ ì°¨ì´ ë§µ
        diff_image = np.abs(original_frames[0].astype(float) - generated_frames[0].astype(float))
        axes[1, 0].imshow(diff_image.astype(np.uint8))
        axes[1, 0].set_title('ì ˆëŒ€ ì°¨ì´ ë§µ')
        axes[1, 0].axis('off')
        
        # ì°¨ì´ íˆíŠ¸ë§µ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼)
        gray_diff = cv2.cvtColor(diff_image.astype(np.uint8), cv2.COLOR_RGB2GRAY)
        im = axes[1, 1].imshow(gray_diff, cmap='hot')
        axes[1, 1].set_title('ì°¨ì´ íˆíŠ¸ë§µ')
        axes[1, 1].axis('off')
        plt.colorbar(im, ax=axes[1, 1])
        
        plt.tight_layout()
        hist_comp_path = self.output_dir / "histogram_comparison.png"
        plt.savefig(hist_comp_path, dpi=150)
        plt.close()
        
        print(f"âœ… í”„ë ˆì„ ë¹„êµ ì €ì¥: {frame_comp_path}")
        print(f"âœ… íˆìŠ¤í† ê·¸ë¨ ë¹„êµ ì €ì¥: {hist_comp_path}")
    
    def analyze_latest_output(self):
        """ê°€ì¥ ìµœê·¼ ìƒì„±ëœ ë¹„ë””ì˜¤ ë¶„ì„"""
        # tmp í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ mp4 íŒŒì¼ ì°¾ê¸°
        tmp_dir = self.project_root / "tmp"
        if not tmp_dir.exists():
            print("âŒ tmp í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        mp4_files = list(tmp_dir.glob("*.mp4"))
        if not mp4_files:
            print("âŒ ìƒì„±ëœ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê°€ì¥ ìµœê·¼ íŒŒì¼
        latest_video = max(mp4_files, key=lambda p: p.stat().st_mtime)
        print(f"ğŸ¬ ë¶„ì„í•  ë¹„ë””ì˜¤: {latest_video}")
        
        self.compare_videos(str(latest_video))


def main():
    parser = argparse.ArgumentParser(description="Ditto TalkingHead ìƒ‰ìƒ ë¹„êµ ë¶„ì„")
    parser.add_argument(
        "--project-root",
        type=str,
        default="/data/edutem/deokjin/ai-human/ditto-talkinghead",
        help="Ditto í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ"
    )
    parser.add_argument(
        "--video",
        type=str,
        help="ë¶„ì„í•  ìƒì„±ëœ ë¹„ë””ì˜¤ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìµœê·¼ íŒŒì¼ ìë™ ì„ íƒ)"
    )
    parser.add_argument(
        "--frames",
        type=int,
        default=5,
        help="ë¹„êµí•  í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )
    
    args = parser.parse_args()
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = DittoColorAnalyzer(args.project_root)
    
    # ë¶„ì„ ì‹¤í–‰
    if args.video:
        analyzer.compare_videos(args.video, args.frames)
    else:
        analyzer.analyze_latest_output()
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()